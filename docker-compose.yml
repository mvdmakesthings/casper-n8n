name: n8n-gpu

networks:
  frontend:
    name: frontend
    driver: bridge
  backend:
    name: backend
    driver: bridge
    internal: true

volumes:
  n8n_data:
    name: n8n-data
  tailscale_state:
    name: tailscale-state

services:
  tailscale:
    container_name: tailscale
    image: tailscale/tailscale:latest
    hostname: ${TS_HOSTNAME:-n8n-gpu}
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY}
      - TS_HOSTNAME=${TS_HOSTNAME:-n8n-gpu}
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_SERVE_CONFIG=/config/serve-config.json
      - TS_USERSPACE=false
    volumes:
      - tailscale_state:/var/lib/tailscale
      - ./tailscale/serve-config.json:/config/serve-config.json:ro
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    devices:
      - /dev/net/tun:/dev/net/tun
    networks:
      - frontend
    restart: unless-stopped

  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    volumes:
      - ${OLLAMA_DATA_PATH:-/mnt/d/Models/ollama}:/root/.ollama
    networks:
      - backend
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  n8n:
    container_name: n8n
    image: n8nio/n8n:latest
    environment:
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
      - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
      - WEBHOOK_URL=${WEBHOOK_URL}
      - N8N_AI_ENABLED=true
    volumes:
      - n8n_data:/home/node/.n8n
    tmpfs:
      - /tmp
      - /home/node/.cache
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "2.0"
    networks:
      - frontend
      - backend
    depends_on:
      tailscale:
        condition: service_started
      ollama:
        condition: service_started
    restart: unless-stopped
