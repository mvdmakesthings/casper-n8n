networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
    internal: true

volumes:
  n8n_data:
  ollama_data:
  tailscale_state:

services:
  tailscale:
    image: tailscale/tailscale:latest
    hostname: ${TS_HOSTNAME:-n8n-gpu}
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY}
      - TS_HOSTNAME=${TS_HOSTNAME:-n8n-gpu}
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_SERVE_CONFIG=/config/serve-config.json
      - TS_USERSPACE=false
    volumes:
      - tailscale_state:/var/lib/tailscale
      - ./tailscale/serve-config.json:/config/serve-config.json:ro
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    devices:
      - /dev/net/tun:/dev/net/tun
    networks:
      - frontend
    restart: unless-stopped

  ollama-init:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - frontend
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        SERVER_PID=$$!
        until ollama list >/dev/null 2>&1; do sleep 2; done
        echo "Pulling llama3.1:8b..."
        ollama pull llama3.1:8b
        echo "Model pull complete."
        kill $$SERVER_PID
        wait $$SERVER_PID 2>/dev/null
        exit 0
    restart: "no"

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - backend
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      ollama-init:
        condition: service_completed_successfully
    restart: unless-stopped

  n8n:
    image: n8nio/n8n:latest
    environment:
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
      - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
      - WEBHOOK_URL=${WEBHOOK_URL}
      - N8N_AI_ENABLED=true
    volumes:
      - n8n_data:/home/node/.n8n
    tmpfs:
      - /tmp
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "2.0"
    networks:
      - frontend
      - backend
    depends_on:
      tailscale:
        condition: service_started
      ollama:
        condition: service_started
    restart: unless-stopped
